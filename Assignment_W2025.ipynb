{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lofillon/WIP/blob/main/Assignment_W2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Machine Learning for Large-Scale Data Analysis and Decision Making  (MATH 60629A) </h1> </center>\n",
        "\n",
        "<center> <h2> WINTER 2025  </h2> </center>\n",
        "\n",
        "<center> <h1> Assignment  </h1> </center>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "20dqTWxVg5Un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[texte du lien](https://)\n",
        "\n",
        "---\n",
        "Deadline    : **February 21, 2025, 11.59 pm EDT**\n",
        "\n",
        "Any instance of plagiarism will be taken very seriously along the lines of the university policy.\n",
        "Refer to the [document](https://www.hec.ca/en/integrity/academic-violations/index.html) for guidelines on plagiarism.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PwqAvHGDg7TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INSTRUCTIONS**\n",
        "\n",
        "* Make a copy of this notebook by selecting `File > Save a copy in Drive`\n",
        "\n",
        "* Rename the copied notebook as your `firstname-lastname-HW`. Add your answers in your notebook.\n",
        "\n",
        "* For text response answers (All of Section 1 and a few others) complete your answer in the text cells below the question.\n",
        " - Look for a tag similar to `<Your answer>`.\n",
        " - **Replace** these tags with your answer.\n",
        " - In some cases table cells with `??` might need to be filled with values to replace the question marks.\n",
        "\n",
        "* For coding questions, complete the code cell and add as many cells as needed. If you perform other tasks which is not purely related to the question, remove them before submitting your work.\n",
        "\n",
        "* For question 2 where external data is needed to complete the task, download the data and load it into **sample_data** folder, which is available from the left bar. Note that everytime the kernel shuts down, you have to re-upload the dataset.\n",
        "\n",
        "* If you look at all the questions in a section in order, everything that you need to complete is obvious based on the tags and descriptions. Make sure not to miss any questions (code and explanation).\n",
        "\n",
        "* You will be graded based on your textual responses as well as the quality of your code.\n",
        "\n",
        "* <font color='red'> Submit a pdf version of this notebook including a read-only link to the notebook. Use with the same naming convention described above. </font>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F01TvkEI90y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Machine Learning Principles (20pt)"
      ],
      "metadata": {
        "id": "xkWNxLOjg7yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 (5pt) Explain the difference between the training error and the generalization error. Make sure to describe how to evaluate the generalization error of a model in practice including pitfalls of this approach."
      ],
      "metadata": {
        "id": "D3E6EzXMg8Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "E8KAjJxfhP0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 (5pt) To increase the size of your training set you first train a model and then use it to obtain labels on an unlabelled test set. You then retrain the model with the data from your train set as well as the data from your test set. Would you expect that your final model would obtain a lower validation error ?"
      ],
      "metadata": {
        "id": "dPL5XifxhP-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "Dpj9GezuhQDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 (5pt) Recall the task of document classification where documents must be classified based on their content. If the documents are encoded in a bag-of-words format, could you use K-NN to classify them? If so, describe a reasonable distance function to use. Otherwise, explain why not."
      ],
      "metadata": {
        "id": "T8iI_AcmhQPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "A37JEUtShwnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 (5pt) Describe both the advantages and the disadvantages of using a larger K when doing K-fold cross validation."
      ],
      "metadata": {
        "id": "b9ugZv2OhwqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "hpm-ZwTKhwsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regression (30pt)"
      ],
      "metadata": {
        "id": "9n8ywxUYhwve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this problem, we will use a sythetic data. The task is to predict the house price given some of the house and distric features. It is a sythetic dataset, so all values are created randomly for this assignment. Therefore, no conclusions must be made based on the results of the tests, analyses and observations.\n"
      ],
      "metadata": {
        "id": "Mi6VkdjphwyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, KFold\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "random_state = 20160202\n",
        "\n",
        "# feel free to add any library that you need to complete your tasks"
      ],
      "metadata": {
        "id": "hI-y5ADjm6U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  #### You can load the data as a dataframe and use .describe() attribute to have a description of the dataset."
      ],
      "metadata": {
        "id": "8-pkzNIDmz2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data-HW\n",
        "!git clone https://github.com/denafiroozi/data-HW"
      ],
      "metadata": {
        "id": "W5pkITUsnUN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Simple Linear Regression and Data Analysis (10pt)"
      ],
      "metadata": {
        "id": "IhZ7gPRUkuLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 What do you notice about the feature values? (2pt)"
      ],
      "metadata": {
        "id": "mcyM5OMohw0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To Do: Use .describe() function and explain your observation below."
      ],
      "metadata": {
        "id": "uIhWzPlwmE0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "VUn57Qrlhw3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Look at the distribution of the feature values by plotting their histograms. What do you notice about each feature? (3pt)"
      ],
      "metadata": {
        "id": "tovf7yMUhw6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To Do: plot the features and explain your observations below. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "MYRRviSAmQ6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "4FErGnGVhw9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Train a linear regression model (in sklearn \"LinearRegression\") with 10-fold cross validation with $R^2$ as the metric. There are multiple correct methods of doing this. If necessary, use the option \"random_state=20160202\" to have reproducible results. What is the average prediction error on the validation sets? (5pt)"
      ],
      "metadata": {
        "id": "mMj8UOIIl-xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "skEKKg7Amq2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "0JmibP4il-0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### 2.2 Two very popular options in Linear Regression are the [Lasso method](https://en.wikipedia.org/wiki/Lasso_(statistics)) and the [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization). These models are implemented in sklearn: [Lasso](http://scikitlearn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) and [Ridge](http://scikitlearn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge). (20pt)\n",
        "\n",
        "#### Hint: If you would like to better understand Lasso, have a look [here](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html)."
      ],
      "metadata": {
        "id": "x2wiOMZ6l-28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 In one or two words describe what Lasso and Ridge capture? (2pt)"
      ],
      "metadata": {
        "id": "3ciFkOOhl-9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "NSg-iuZDnz1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Train a Lasso model and a Ridge model using 5-fold cross validation. Compare the weights of the attributes (features) between Lasso, Ridge, and the Linear Regression models. What do you notice? (13pt)"
      ],
      "metadata": {
        "id": "Mi_-YvBdnz3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "iO6VbGVYoGhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "E-FizxCHnz84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 What is the performance of the three models on the validation sets (averaged across folds)? Which model performs the best? (5pt)"
      ],
      "metadata": {
        "id": "56UbKAAwnz_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "0bM6RE3NoKUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fill in the table by replacing `??`s with the values you observed in the above function calls.\n",
        "|    model    |   CV score (R2)   |\n",
        "|:------:|:------:|\n",
        "|   LinearRegression  |  ?? |\n",
        "|  Ridge |  ??  |\n",
        "| Lasso |  ??  |"
      ],
      "metadata": {
        "id": "nHgWeF6koXT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer to the second part of 2.2.3 comes here>`"
      ],
      "metadata": {
        "id": "7LXKGCs2qn0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Classification (70pt)"
      ],
      "metadata": {
        "id": "BqcAQaPBX8R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this question, you will train a Naive Bayes (NB) model, Support Vector Machine (SVM), and a Neural Network for a text classification task.\n",
        "\n",
        "#### You will use a sklearn dataset that consists of around 18000 newsgroups posts on 20 topics. You can read more about the function [here](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) and how to load the data [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html).\n",
        "\n",
        "#### Among all the topics, our focus is on \"rec.sport.baseball\" and \"rec.sport.hockey\". Use \"categories\" to select these two topics and set the \"subset\" to \"all\". Then, split the data twice to obtain train, validation and test sets using random_state 42, with 70% of the data in the training set, and the rest being divided equally into test and validation sets."
      ],
      "metadata": {
        "id": "2r6Yi07nX8Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data Pre-processing (5pt)"
      ],
      "metadata": {
        "id": "T0zCDPvOX8Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First obtain the Bag-of-Word (BoW) representation  and the TF-IDF representation for the dataset. Sklearn provides functions to obtain these representations (See [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for BoW and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for TF-IDF representations). To limit the required training time, we ask that you keep a maximum of 200 words (you may explore different values) for your vocabulary (max\\_features=200). Also, set decode_error to \"replace\", strip_accents to \"unicode\", and stop_words to \"english\". Do not change the other parameters of the sklearn functions. Feel free to make any conversions you deem appropriate."
      ],
      "metadata": {
        "id": "9-95OivtX8hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from sklearn.naive_bayes import ComplementNB, GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "JuqYr_JBf6RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the data below."
      ],
      "metadata": {
        "id": "89hKVbbY4xql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_all = fetch_20newsgroups(subset='all', categories = <list of two categories>)"
      ],
      "metadata": {
        "id": "F3EGk7cpf8Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The texts and binary labels are accessed by data_all.data and data_all.taregt, respectively. To get the names of labels use data_all.target_names.\n",
        "\n",
        "Then, convert the lists into numpy arrays for future manipulations."
      ],
      "metadata": {
        "id": "8YrEvPgy48KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = np.array(data_all.data)\n",
        "y = np.array(newsgroups_all.target)"
      ],
      "metadata": {
        "id": "uXHRoMEegCpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we perform lemmetization on X_raw."
      ],
      "metadata": {
        "id": "pBcHwQMPXEGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "QknDW_H1XRoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.Series(X_raw).apply(lambda x: \" \".join(lemmatizer.lemmatize(word, \"v\") for word in x.split()))"
      ],
      "metadata": {
        "id": "GoZXyxzJXEnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do not forget to split the data into train, validation and test sets before training the model."
      ],
      "metadata": {
        "id": "w0kQKia8YIAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you need to derive the associated representations."
      ],
      "metadata": {
        "id": "iOIZTW7yXcJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Bag-of-Words (3pt)"
      ],
      "metadata": {
        "id": "fq4i89N0bD17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "sbw2_XrNbIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 TF-IDF (2pt)"
      ],
      "metadata": {
        "id": "pUhV1zRobLwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "Hbk7AVPdbQYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Naive Bayes (15 pt)"
      ],
      "metadata": {
        "id": "pHB2XmbMbZm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 For each representation (bag of words and TF-IDF) train the appropriate Naive Bayes model for the task. Which Naive Bayes model did you use? What is the performance of each model on the validation set in terms of classification accuracy? (5pt)"
      ],
      "metadata": {
        "id": "fMt6p0r3b17e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "9iHOcnhhb8lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "D6IB8cdWcB2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Using the model trained with the Bag-of-Words representation and for each class, which are the 5 words with the highest impact on the classification? (10 pt)"
      ],
      "metadata": {
        "id": "1DrYkuZbbo7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "Z1W8mShxb_AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "6TLq9EBncHu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 SVM (10 pt)"
      ],
      "metadata": {
        "id": "rJXiZhKrd_wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this section, you will learn about kernel functions for SVM. A kernel function is a function that is applied on the input to transform it before it is fed to the SVM. Data that is not linearly separable can be transformed to a new feature space where every data point is now linearly separable. But how do we pick this feature map? Kernel functions simplify this process because we do not need to explicitly model the feature transformation to a new space. We can instead use it to implicitly transform the features to a new space and hopefully make the data more linearly separable."
      ],
      "metadata": {
        "id": "NV6S9tYRd_zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As an example, the kernel function $k(x, y) = (x \\cdot y)^2$ implicitly defines a feature map $\\phi$ if it satisfies the condition $k(x, y) = \\phi(x) \\cdot \\phi(y)$. The function $\\phi$ transforms the features to a higher-dimensional space. The kernel function, on the other hand, only performs operations in the lower-dimensional original space. Learning SVMs with these kernels can be much more efficient. For more information about SMVs and kernel functions, you can read this [article](https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f)."
      ],
      "metadata": {
        "id": "hiYhYv38e1HS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Your task is to fit several SVM models to the data and pick the best one."
      ],
      "metadata": {
        "id": "_414X8AQd_3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Consider the following values/cases for the regularization parameter and kernel, respectively:\n",
        "* C = [0.1, 1, 10]\n",
        "* kernel = [linear, rbf, poly]\n",
        "\n",
        "*Remark*: For more information about the Radial basis function (rbf) kernel and the polynomial (poly) kernel see, respectively, [here](https://en.wikipedia.org/wiki/Radial_basis_function_kernel#:~:text=In%20machine%20learning%2C%20the%20radial,in%20support%20vector%20machine%20classification.) and\n",
        " [here](https://en.wikipedia.org/wiki/Polynomial_kernel)."
      ],
      "metadata": {
        "id": "eouEnHDze8eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 Train the model using the train data and find the model that has the best performance on the validation set. Which kernel and which C parameter did you choose? What is the best model's performance on the test data? (10 pt)"
      ],
      "metadata": {
        "id": "xI0I6IIrfh4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "ZGklssoogAig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`\n",
        "\n"
      ],
      "metadata": {
        "id": "6H4_Xf4MgAxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Neural Network (30pt)"
      ],
      "metadata": {
        "id": "DaE3ne_Dbo-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### When you instantiate your neural network, you must set the random seed to 12345 (in other words random\\_state=12345)."
      ],
      "metadata": {
        "id": "ZbNvy8oEbpBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For the numerical representations you obtained in the first part, you will now train neural networks with different hyperparameters. Use the option early\\_stopping=True and train neural networks with all combinations of the following hyperparameters:\n",
        "\n",
        "* size of first hidden layer : [4,8,16]\n",
        "* size of second hidden layer : [0, 4, 8].\n",
        "* learning rate: [0.1, 0.01, 0.001]\n",
        "* L2 regularization strength: [0.001, 0.01, 0.1]\n"
      ],
      "metadata": {
        "id": "-NyH-ewlbpFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.1 What is the best combination of hyperparameters and what performance does it obtain on the validation set? (15pt)"
      ],
      "metadata": {
        "id": "ERGT_elybpIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed."
      ],
      "metadata": {
        "id": "pKJqaXWtdizr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`\n",
        "\n"
      ],
      "metadata": {
        "id": "QB7fSJw1bpMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2 What did you notice regarding the importance of the different hyperparameters you tried? Use graphs to show the change in training and validation errors when hyperparameters change (15pt)"
      ],
      "metadata": {
        "id": "cm7tYUv3dwjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code comes here. Feel free to add as many code cells as needed. If you have already done a complete analysis on the code cell above and this code cell is no longer needed, please leave it empty."
      ],
      "metadata": {
        "id": "ny5TjraZdwu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "cvaJ9xVRdw5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Comparison (10pt)"
      ],
      "metadata": {
        "id": "3U355HcmetOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 Which model, NB, SVM or Neural Network, is the best? Justify your answer. (10pt)"
      ],
      "metadata": {
        "id": "6MSjWnBxetsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `<Your answer>`"
      ],
      "metadata": {
        "id": "STlDJZ5re4AY"
      }
    }
  ]
}